{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b80c56-5fe8-4f6e-b14b-1e6bd087f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d4402d-8037-45a2-9a27-be9201faf1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 265\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaa6a27-6770-4325-a525-67f90bd00191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MyMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(32 * 32 * 3, 512)\n",
    "        self.h1 = nn.Linear(512, 128)\n",
    "        self.h2 = nn.Linear(128, 32)\n",
    "        self.output_layer = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.flatten(x, 1)\n",
    "        out = F.relu(self.input_layer(out))\n",
    "        out = F.relu(self.h1(out))\n",
    "        out = F.relu(self.h2(out))\n",
    "        out = self.output_layer(out)\n",
    "        return out\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"MyMLP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6b085b-5d2c-42d3-a0a6-7ecc6d7bc852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_CIFAR2(train_val_split=.9, data_path='../data', preprocessor=None):\n",
    "    if preprocessor is None:\n",
    "        preprocessor = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    data_train_val = datasets.CIFAR10(\n",
    "        data_path,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=preprocessor)\n",
    "    \n",
    "    data_test = datasets.CIFAR10(\n",
    "        data_path,\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=preprocessor)\n",
    "\n",
    "    n_train = int(len(data_train_val)*train_val_split)\n",
    "    n_val = len(data_train_val) - n_train\n",
    "\n",
    "    data_train, data_val = random_split(\n",
    "        data_train_val,\n",
    "        [n_train, n_val],\n",
    "        generator=torch.Generator()\n",
    "    )\n",
    "    \n",
    "    label_map = {0: 0, 2: 1}\n",
    "    class_names = ['airplane', 'bird']\n",
    "    \n",
    "    data_train = [(img, label_map[label]) for img, label in data_train if label in [0, 2]]\n",
    "    data_val = [(img, label_map[label]) for img, label in data_val if label in [0, 2]]\n",
    "    data_test = [(img, label_map[label]) for img, label in data_test if label in [0, 2]]\n",
    "\n",
    "    print(\"Size of training set: \", len(data_train))\n",
    "    print(\"Size of validation set: \", len(data_val))\n",
    "    print(\"Size of test set: \", len(data_test))\n",
    "\n",
    "    return (data_train, data_val, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904bfafe-5ffd-4795-bba2-52be61c4c679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs: int, optimizer: optim.Optimizer, model: nn.Module, loss_fn, train_loader: DataLoader, val_loader: DataLoader):\n",
    "    \"\"\"\n",
    "    Train a model with pytorch optimizer.\n",
    "    \"\"\"\n",
    "    print(f\"Training {model} with optimizer\")\n",
    "    n_batch = len(train_loader)\n",
    "    losses_train = []\n",
    "    losses_val = []\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        model.train()\n",
    "        loss_train = 0.0\n",
    "            \n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=DEVICE, dtype=torch.double)\n",
    "            labels = labels.to(device=DEVICE)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "\n",
    "            loss = loss_fn(outputs,labels)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        losses_train.append(loss_train / n_batch)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            loss_val = 0.0\n",
    "            for imgs, labels in val_loader:\n",
    "                imgs = imgs.to(device=DEVICE, dtype=torch.double)\n",
    "                labels = labels.to(device=DEVICE)\n",
    "                outputs = model(imgs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "\n",
    "                loss_val += loss.item()\n",
    "            losses_val.append(loss_val / n_batch)\n",
    "                \n",
    "\n",
    "        if epoch == 1 or epoch % 5 == 0:\n",
    "            print(f\"{datetime.now().time()}, {epoch}, train_loss: {loss_train/n_batch:.5f}, val_loss: {loss_val/n_batch:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a1a24c-5529-4fea-945c-2b169d922f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_manual_update(n_epochs: int, lr: float, model: nn.Module, loss_fn, train_loader: DataLoader, val_loader: DataLoader, weight_decay=0, momentum=0):\n",
    "    \"\"\"\n",
    "    Train a model with manual update to parameters.\n",
    "    \"\"\"\n",
    "    print(f\"Training {model} with manual update\")\n",
    "    n_batch = len(train_loader)\n",
    "    losses_train = []\n",
    "    losses_val = []\n",
    "\n",
    "    # Momentum buffer\n",
    "    m_b = [torch.zeros_like(p) for p in model.parameters()] \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        model.train()\n",
    "        loss_train = 0.0\n",
    "\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=DEVICE, dtype=torch.double)\n",
    "            labels = labels.to(device=DEVICE)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs,labels)\n",
    "            loss.backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for i, p in enumerate(model.parameters()):\n",
    "                    # L2 regularization\n",
    "                    if weight_decay != 0:\n",
    "                        p.grad = p.grad + weight_decay*p.data\n",
    "                    # Momentum\n",
    "                    if momentum != 0:\n",
    "                        m_b[i] = (momentum*m_b[i]) + p.grad\n",
    "                        p.grad = m_b[i]\n",
    "                        \n",
    "                    # Gradient step\n",
    "                    new_params = p.data-lr*p.grad\n",
    "                    p.copy_(new_params)\n",
    "                \n",
    "                        \n",
    "                model.zero_grad()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        losses_train.append(loss_train / n_batch)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            loss_val = 0.0\n",
    "            for imgs, labels in val_loader:\n",
    "                imgs = imgs.to(device=DEVICE, dtype=torch.double)\n",
    "                labels = labels.to(device=DEVICE)\n",
    "                outputs = model(imgs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "\n",
    "                loss_val += loss.item()\n",
    "            losses_val.append(loss_val / n_batch)\n",
    "                \n",
    "\n",
    "        if epoch == 1 or epoch % 5 == 0:\n",
    "            print(f\"{datetime.now().time()}, {epoch}, train_loss: {loss_train/n_batch:.5f}, val_loss: {loss_val/n_batch:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706669f3-682a-4eed-bbda-8ebab767bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs = imgs.to(device=DEVICE)\n",
    "            labels = labels.to(device=DEVICE)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += labels.shape[0]\n",
    "            correct += int((predicted == labels).sum())\n",
    "    acc = correct / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73212d55-86a4-4201-815f-216ec049012c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(n_epochs: int, batch_size: int, train_loader: DataLoader, val_loader: DataLoader):\n",
    "    \"\"\"\n",
    "    Trains and compares the two training functions.\n",
    "    \"\"\"\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    seed = SEED\n",
    "\n",
    "    print(\"\\tGlobal parameters:\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Epochs: {n_epochs}\")\n",
    "    print(f\"Loss function: {loss_fn}\")\n",
    "    print(f\"Seed: {seed}\")\n",
    "    \n",
    "    hyper_params = [\n",
    "        {\"lr\": 0.01, \"weight_decay\": 0, \"momentum\": 0},\n",
    "        {\"lr\": 0.01, \"weight_decay\": 0.01, \"momentum\": 0},\n",
    "        {\"lr\": 0.01, \"weight_decay\": 0, \"momentum\": 0.01},\n",
    "        {\"lr\": 0.01, \"weight_decay\": 0, \"momentum\": 0.9},\n",
    "        {\"lr\": 0.01, \"weight_decay\": 0.01, \"momentum\": 0.9},\n",
    "        {\"lr\": 0.001, \"weight_decay\": 0.01, \"momentum\": 0.9},\n",
    "    ]\n",
    "\n",
    "    models = []\n",
    "    accuracies = []\n",
    "\n",
    "    # Train both models with the different hyperparameters\n",
    "    for hparam in hyper_params:\n",
    "        print(\"\\n\", \"=\"*50)\n",
    "        print(\"\\tCurrent parameters: \")\n",
    "        [print(f\"{key}:{value}\") for key, value in hparam.items()]\n",
    "\n",
    "        print(\"\\n\", \"-\"*6, \"Using pytorch SGD\", \"-\"*6)\n",
    "        torch.manual_seed(SEED)\n",
    "        model_auto = MyMLP()\n",
    "        model_auto.to(device=DEVICE)\n",
    "        optimizer = optim.SGD(model_auto.parameters(), **hparam)\n",
    "        train(n_epochs, optimizer, model_auto, loss_fn, train_loader, val_loader)\n",
    "        train_acc_auto = compute_accuracy(model_auto, train_loader)\n",
    "        val_acc_auto = compute_accuracy(model_auto, val_loader)\n",
    "\n",
    "        models.append(model_auto)\n",
    "        accuracies.append(val_acc_auto)\n",
    "        \n",
    "        print(\"\\n\", \"-\"*3, \"Accuracies\", \"-\"*3)\n",
    "        print(f\"Training accuracy: {train_acc_auto:.2f}\")\n",
    "        print(f\"Validation accuracy: {val_acc_auto:.2f}\")\n",
    "        \n",
    "        print(\"\\n\", \"-\"*6, \"Using manual update\", \"-\"*6)\n",
    "        torch.manual_seed(SEED)\n",
    "        model_manual = MyMLP()\n",
    "        model_manual.to(device=DEVICE)\n",
    "        train_manual_update(n_epochs, model=model_manual, train_loader=train_loader, val_loader=val_loader, loss_fn=loss_fn, **hparam)\n",
    "        train_acc_manual = compute_accuracy(model_manual, train_loader)\n",
    "        val_acc_manual = compute_accuracy(model_manual, val_loader)\n",
    "        \n",
    "        models.append(model_manual)\n",
    "        accuracies.append(val_acc_manual)\n",
    "        \n",
    "        print(\"\\n\", \"-\"*3, \"Accuracies\", \"-\"*3)\n",
    "        print(f\"Training accuracy: {train_acc_manual:.2f}\")\n",
    "        print(f\"Validation accuracy: {val_acc_manual:.2f}\")\n",
    "\n",
    "\n",
    "    return models, accuracies, hyper_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3c9bb2-1120-464f-91b6-ddaed08a14ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add normalization from training data\n",
    "tensor_train, _, _ = load_CIFAR2()\n",
    "imgs = torch.stack([img for img, _ in tensor_train])\n",
    "print(imgs.shape)\n",
    "\n",
    "normalizer = transforms.Normalize(\n",
    "    imgs.mean(dim=(0,2,3)),\n",
    "    imgs.std(dim=(0,2,3))\n",
    ")\n",
    "\n",
    "preprocessor = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalizer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa05f25-e8c4-46d4-af19-23d1316428d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_train, cifar_val, cifar_test = load_CIFAR2(preprocessor=preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6564caf-1d37-4665-8d2e-e6f828f55ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(cifar_train, shuffle=False, batch_size=batch_size)\n",
    "val_loader = DataLoader(cifar_val, shuffle=False, batch_size=batch_size)\n",
    "models, accuracies, hyper_params = compare_models(n_epochs, batch_size, train_loader=train_loader, val_loader=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aa42d1-e1c9-4344-bf5c-d81dffe816b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_acc = max(accuracies)\n",
    "selected_idx = accuracies.index(highest_acc)\n",
    "selected_model = models[selected_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712a0d31-b8fc-4a89-8d79-fc3c9f535df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(cifar_test, shuffle=False, batch_size=batch_size)\n",
    "acc = compute_accuracy(selected_model, test_loader)\n",
    "print(f\"Used hyperparameters: {hyper_params[selected_idx//2]}\")\n",
    "print(f\"Accuracy of selected model: {acc:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
