{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5b80c56-5fe8-4f6e-b14b-1e6bd087f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from collections import Counter\n",
    "from classes.MyMLP import MyMLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63d4402d-8037-45a2-9a27-be9201faf1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "SEED = 808\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d6b085b-5d2c-42d3-a0a6-7ecc6d7bc852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Size of training set:  8956\n",
      "Size of validation set:  1044\n",
      "Size of test set:  2000\n"
     ]
    }
   ],
   "source": [
    "def load_CIFAR2(train_val_split=.9, data_path='data', preprocessor=None):\n",
    "    if preprocessor is None:\n",
    "        preprocessor = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    data_train_val = datasets.CIFAR10(\n",
    "        data_path,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=preprocessor)\n",
    "    \n",
    "    data_test = datasets.CIFAR10(\n",
    "        data_path,\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=preprocessor)\n",
    "\n",
    "    n_train = int(len(data_train_val)*train_val_split)\n",
    "    n_val = len(data_train_val) - n_train\n",
    "\n",
    "    data_train, data_val = random_split(\n",
    "        data_train_val,\n",
    "        [n_train, n_val],\n",
    "        generator=torch.Generator()\n",
    "    )\n",
    "    \n",
    "    label_map = {0: 0, 2: 1}\n",
    "    class_names = ['airplane', 'bird']\n",
    "    \n",
    "    data_train = [(img, label_map[label]) for img, label in data_train if label in [0, 2]]\n",
    "    data_val = [(img, label_map[label]) for img, label in data_val if label in [0, 2]]\n",
    "    data_test = [(img, label_map[label]) for img, label in data_test if label in [0, 2]]\n",
    "\n",
    "    print(\"Size of training set: \", len(data_train))\n",
    "    print(\"Size of validation set: \", len(data_val))\n",
    "    print(\"Size of test set: \", len(data_test))\n",
    "\n",
    "    return (data_train, data_val, data_test)\n",
    "\n",
    "cifar_train, cifar_val, cifar_test = load_CIFAR2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "904bfafe-5ffd-4795-bba2-52be61c4c679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    print(f\"Training {model} with optimizer\")\n",
    "    n_batch = len(train_loader)\n",
    "    losses_train = []\n",
    "    losses_val = []\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        model.train()\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=DEVICE, dtype=torch.double)\n",
    "            labels = labels.to(device=DEVICE)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "\n",
    "            loss = loss_fn(outputs,labels)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        losses_train.append(loss_train / n_batch)\n",
    "\n",
    "        # if epoch == 1 or epoch % 10 == 0:\n",
    "        print(f\"{datetime.now().time()}, {epoch}, train_loss: {loss/n_batch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70a1a24c-5529-4fea-945c-2b169d922f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, loss, learning_rate):\n",
    "    return parameters - learning_rate*gradients\n",
    "    \n",
    "def train_manual_update(n_epochs, lr, model, loss_fn, train_loader):\n",
    "    print(f\"Training {model} with manual update\")\n",
    "    n_batch = len(train_loader)\n",
    "    losses_train = []\n",
    "    losses_val = []\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        model.train()\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=DEVICE, dtype=torch.double)\n",
    "            labels = labels.to(device=DEVICE)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "\n",
    "            loss = loss_fn(outputs,labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # Implement gradient descent here:\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    new_values = update_parameters(p, p.grad, loss, learning_rate)\n",
    "                    p.copy_(new_values)\n",
    "                model.zero_grad()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        losses_train.append(loss_train / n_batch)\n",
    "\n",
    "        # if epoch == 1 or epoch % 10 == 0:\n",
    "        print(f\"{datetime.now().time()}, {epoch}, train_loss: {loss/n_batch}\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "480a6075-42ef-47a7-ae7f-b506624302dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyMLP(\n",
       "  (input_layer): Linear(in_features=3072, out_features=512, bias=True)\n",
       "  (h1): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (h2): Linear(in_features=128, out_features=32, bias=True)\n",
       "  (output_layer): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "torch.manual_seed(SEED)\n",
    "model_auto = MyMLP()\n",
    "model_auto.to(device=DEVICE)\n",
    "optimizer = optim.SGD(model_auto.parameters(), lr=learning_rate)\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "model_manual = MyMLP()\n",
    "model_manual.to(device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0998fb18-c6ca-45dc-ad35-9a5eaa5aa118",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(cifar_train, shuffle=False, batch_size=1)\n",
    "val_loader = DataLoader(cifar_val, shuffle=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50f21f7b-74bf-4959-86a7-398328c6ce3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MyMLP with optimizer\n",
      "11:05:39.899756, 1, train_loss: 0.00010952912793141069\n",
      "11:05:45.042024, 2, train_loss: 5.283247569553806e-05\n",
      "11:05:50.331483, 3, train_loss: 4.344831406477836e-05\n",
      "11:05:55.856810, 4, train_loss: 2.7311411381614114e-05\n",
      "11:06:01.160560, 5, train_loss: 2.3984670643872053e-05\n",
      "11:06:07.202230, 6, train_loss: 2.1332854115226365e-05\n",
      "11:06:13.713791, 7, train_loss: 2.0628499053169388e-05\n",
      "11:06:20.255842, 8, train_loss: 2.329091972699752e-05\n",
      "11:06:26.401080, 9, train_loss: 2.073580252017654e-05\n",
      "11:06:32.619339, 10, train_loss: 2.7415180050618997e-05\n",
      "Training MyMLP with manual update\n",
      "11:06:38.914601, 1, train_loss: 0.00010952912793141069\n",
      "11:06:45.044169, 2, train_loss: 5.283247569553807e-05\n",
      "11:06:51.077819, 3, train_loss: 4.3448314064778376e-05\n",
      "11:06:57.113550, 4, train_loss: 2.7311411381614114e-05\n",
      "11:07:03.148714, 5, train_loss: 2.3984670643872053e-05\n",
      "11:07:09.221421, 6, train_loss: 2.1332854115226365e-05\n",
      "11:07:15.265427, 7, train_loss: 2.0628499053169388e-05\n",
      "11:07:21.348693, 8, train_loss: 2.329091972699752e-05\n",
      "11:07:27.403556, 9, train_loss: 2.07358025201765e-05\n",
      "11:07:35.208202, 10, train_loss: 2.7415180050618936e-05\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "train(n_epochs, optimizer, model_auto, loss_fn, train_loader)\n",
    "train_manual_update(n_epochs, learning_rate, model_manual, loss_fn, train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
