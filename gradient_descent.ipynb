{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5b80c56-5fe8-4f6e-b14b-1e6bd087f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from collections import Counter\n",
    "from classes.MyMLP import MyMLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63d4402d-8037-45a2-9a27-be9201faf1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "SEED = 808\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d6b085b-5d2c-42d3-a0a6-7ecc6d7bc852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Size of training set:  8956\n",
      "Size of validation set:  1044\n",
      "Size of test set:  2000\n"
     ]
    }
   ],
   "source": [
    "def load_CIFAR2(train_val_split=.9, data_path='data', preprocessor=None):\n",
    "    if preprocessor is None:\n",
    "        preprocessor = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    data_train_val = datasets.CIFAR10(\n",
    "        data_path,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=preprocessor)\n",
    "    \n",
    "    data_test = datasets.CIFAR10(\n",
    "        data_path,\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=preprocessor)\n",
    "\n",
    "    n_train = int(len(data_train_val)*train_val_split)\n",
    "    n_val = len(data_train_val) - n_train\n",
    "\n",
    "    data_train, data_val = random_split(\n",
    "        data_train_val,\n",
    "        [n_train, n_val],\n",
    "        generator=torch.Generator()\n",
    "    )\n",
    "    \n",
    "    label_map = {0: 0, 2: 1}\n",
    "    class_names = ['airplane', 'bird']\n",
    "    \n",
    "    data_train = [(img, label_map[label]) for img, label in data_train if label in [0, 2]]\n",
    "    data_val = [(img, label_map[label]) for img, label in data_val if label in [0, 2]]\n",
    "    data_test = [(img, label_map[label]) for img, label in data_test if label in [0, 2]]\n",
    "\n",
    "    print(\"Size of training set: \", len(data_train))\n",
    "    print(\"Size of validation set: \", len(data_val))\n",
    "    print(\"Size of test set: \", len(data_test))\n",
    "\n",
    "    return (data_train, data_val, data_test)\n",
    "\n",
    "cifar_train, cifar_val, cifar_test = load_CIFAR2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "904bfafe-5ffd-4795-bba2-52be61c4c679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    print(f\"Training {model} with optimizer\")\n",
    "    n_batch = len(train_loader)\n",
    "    losses_train = []\n",
    "    losses_val = []\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        model.train()\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=DEVICE, dtype=torch.double)\n",
    "            labels = labels.to(device=DEVICE)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "\n",
    "            loss = loss_fn(outputs,labels)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        losses_train.append(loss_train / n_batch)\n",
    "\n",
    "        if epoch == 1 or epoch % 5 == 0:\n",
    "            print(f\"{datetime.now().time()}, {epoch}, train_loss: {loss/n_batch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70a1a24c-5529-4fea-945c-2b169d922f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_manual_update(n_epochs, lr, model, loss_fn, train_loader, weight_decay=0, momentum=0):\n",
    "    print(f\"Training {model} with manual update\")\n",
    "    n_batch = len(train_loader)\n",
    "    losses_train = []\n",
    "    losses_val = []\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        model.train()\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=DEVICE, dtype=torch.double)\n",
    "            labels = labels.to(device=DEVICE)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs,labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # Implement gradient descent here:\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    g_t = p.grad\n",
    "                    if weight_decay != 0:\n",
    "                        g_t = g_t + weight_decay*p.data\n",
    "\n",
    "                    # if momentum != 0:\n",
    "                    #     b_t_prev = None\n",
    "                    #     if b_t_prev is not None:\n",
    "                    #         b_t = momentum*b_t_prev + g_t\n",
    "                    #     else:\n",
    "                    #         b_t = g_t\n",
    "                    #         b_t_prev = b_t\n",
    "\n",
    "                    #     g_t = b_t\n",
    "                    new_params = p.data-lr*g_t\n",
    "                    p.copy_(new_params)\n",
    "                \n",
    "                        \n",
    "                model.zero_grad()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        losses_train.append(loss_train / n_batch)\n",
    "\n",
    "        if epoch == 1 or epoch % 5 == 0:\n",
    "            print(f\"{datetime.now().time()}, {epoch}, train_loss: {loss/n_batch}\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "706669f3-682a-4eed-bbda-8ebab767bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs = imgs.to(device=DEVICE)\n",
    "            labels = labels.to(device=DEVICE)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += labels.shape[0]\n",
    "            correct += int((predicted == labels).sum())\n",
    "    acc = correct / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73212d55-86a4-4201-815f-216ec049012c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(n_epochs, batch_size):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    seed = SEED\n",
    "\n",
    "    print(\"\\tGlobal parameters:\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Epochs: {n_epochs}\")\n",
    "    print(f\"Loss function: {loss_fn}\")\n",
    "    print(f\"Seed: {seed}\")\n",
    "    \n",
    "    train_loader = DataLoader(cifar_train, shuffle=False, batch_size=batch_size)\n",
    "    val_loader = DataLoader(cifar_val, shuffle=False, batch_size=batch_size)\n",
    "    \n",
    "    hyper_params = [\n",
    "        {\"lr\": 0.01, \"weight_decay\": 0, \"momentum\": 0},\n",
    "        {\"lr\": 0.01, \"weight_decay\": 0.01, \"momentum\": 0},\n",
    "    ]\n",
    "\n",
    "    models = []\n",
    "    accuracies = []\n",
    "\n",
    "    for hparam in hyper_params:\n",
    "        print(\"\\n\", \"=\"*50)\n",
    "        print(\"\\tCurrent parameters: \")\n",
    "        [print(f\"{key}: {value}\") for key, value in hparam.items()]\n",
    "\n",
    "        print(\"\\n\", \"-\"*6, \"Using pytorch SGD\", \"-\"*6)\n",
    "        torch.manual_seed(SEED)\n",
    "        model_auto = MyMLP()\n",
    "        model_auto.to(device=DEVICE)\n",
    "        optimizer = optim.SGD(model_auto.parameters(), **hparam)\n",
    "        train(n_epochs, optimizer, model_auto, loss_fn, train_loader)\n",
    "        train_acc_auto = compute_accuracy(model_auto, train_loader)\n",
    "        val_acc_auto = compute_accuracy(model_auto, val_loader)\n",
    "\n",
    "        models.append(model_auto)\n",
    "        accuracies.append(val_acc_auto)\n",
    "        \n",
    "        print(\"\\n\", \"-\"*3, \"Accuracies\", \"-\"*3)\n",
    "        print(f\"Training accuracy: {train_acc_auto:.2f}\")\n",
    "        print(f\"Validation accuracy: {val_acc_auto:.2f}\")\n",
    "        \n",
    "        print(\"\\n\", \"-\"*6, \"Using manual update\", \"-\"*6)\n",
    "        torch.manual_seed(SEED)\n",
    "        model_manual = MyMLP()\n",
    "        model_manual.to(device=DEVICE)\n",
    "        train_manual_update(n_epochs, model=model_manual, train_loader=train_loader, loss_fn=loss_fn, **hparam)\n",
    "        train_acc_manual = compute_accuracy(model_manual, train_loader)\n",
    "        val_acc_manual = compute_accuracy(model_manual, val_loader)\n",
    "        \n",
    "        models.append(model_manual)\n",
    "        accuracies.append(val_acc_manual)\n",
    "        \n",
    "        print(\"\\n\", \"-\"*3, \"Accuracies\", \"-\"*3)\n",
    "        print(f\"Training accuracy: {train_acc_manual:.2f}\")\n",
    "        print(f\"Validation accuracy: {val_acc_manual:.2f}\")\n",
    "\n",
    "\n",
    "    return models, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6564caf-1d37-4665-8d2e-e6f828f55ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tGlobal parameters:\n",
      "Batch size: 1\n",
      "Epochs: 1\n",
      "Loss function: CrossEntropyLoss()\n",
      "Seed: 808\n",
      "\n",
      " ==================================================\n",
      "\tCurrent parameters: \n",
      "lr: 0.01\n",
      "weight_decay: 0\n",
      "momentum: 0\n",
      "\n",
      " ------ Using pytorch SGD ------\n",
      "Training MyMLP with optimizer\n",
      "23:00:45.980795, 1, train_loss: 0.00010952912793141069\n",
      "\n",
      " --- Accuracies ---\n",
      "Training accuracy: 0.79\n",
      "Validation accuracy: 0.77\n",
      "\n",
      " ------ Using manual update ------\n",
      "Training MyMLP with manual update\n",
      "23:00:54.610955, 1, train_loss: 0.00010952912793141069\n",
      "\n",
      " --- Accuracies ---\n",
      "Training accuracy: 0.79\n",
      "Validation accuracy: 0.77\n",
      "\n",
      " ==================================================\n",
      "\tCurrent parameters: \n",
      "lr: 0.01\n",
      "weight_decay: 0.01\n",
      "momentum: 0\n",
      "\n",
      " ------ Using pytorch SGD ------\n",
      "Training MyMLP with optimizer\n",
      "23:01:01.886148, 1, train_loss: 0.000131260954048647\n",
      "\n",
      " --- Accuracies ---\n",
      "Training accuracy: 0.78\n",
      "Validation accuracy: 0.77\n",
      "\n",
      " ------ Using manual update ------\n",
      "Training MyMLP with manual update\n",
      "23:01:12.019116, 1, train_loss: 0.000131260954048647\n",
      "\n",
      " --- Accuracies ---\n",
      "Training accuracy: 0.78\n",
      "Validation accuracy: 0.77\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "batch_size = 1\n",
    "models, accuracies = compare_models(n_epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7aa42d1-e1c9-4344-bf5c-d81dffe816b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_acc = max(accuracies)\n",
    "selected_model = models[accuracies.index(highest_acc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "712a0d31-b8fc-4a89-8d79-fc3c9f535df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of selected model: 0.79\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(cifar_test, shuffle=False, batch_size=batch_size)\n",
    "acc = compute_accuracy(selected_model, test_loader)\n",
    "print(f\"Accuracy of selected model: {acc:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
